{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run this notebook on Google Colab after running the scrpe_links.py script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uC5SphHDH23u",
        "outputId": "196592fb-5922-4883-f7b4-f66343ddff0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1pvzYFByIHZn"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt install -y chromium-chromedriver\n",
        "!pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "L-_hnEZEI2fk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Specify the directory you want to list the files in your Google Drive root\n",
        "directory = '/content/drive/MyDrive/'\n",
        "\n",
        "# List all files and directories in the specified path\n",
        "files = os.listdir(directory)\n",
        "\n",
        "# Print all files\n",
        "for file in files:\n",
        "    print(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MFtmXniTnIoG"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from google.colab import drive\n",
        "import time\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def scrape_project_data(url):\n",
        "    try:\n",
        "        # Setup Chrome options for Colab\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument(\"--headless\")\n",
        "        chrome_options.add_argument(\"--no-sandbox\")\n",
        "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "        # Initialize the WebDriver\n",
        "        driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "        driver.get(url)\n",
        "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CLASS_NAME, 'gemini-type-h1')))\n",
        "\n",
        "        # Scraping the title\n",
        "        title_element = driver.find_element(By.CLASS_NAME, 'gemini-type-h1')\n",
        "        title = title_element.text.strip() if title_element else \"N/A\"\n",
        "\n",
        "        # Scraping the sub-title (description under the title)\n",
        "        try:\n",
        "            sub_title_element = driver.find_element(By.CLASS_NAME, 'gemini-type-t1')\n",
        "            sub_title = sub_title_element.text.strip() if sub_title_element else \"N/A\"\n",
        "        except:\n",
        "            sub_title = \"N/A\"\n",
        "            print(f\"Subtitle not found for {url}\")\n",
        "\n",
        "        # Initialize variables\n",
        "        video_link = \"N/A\"\n",
        "\n",
        "        # Retry mechanism for YouTube link extraction\n",
        "        for _ in range(3): \n",
        "            try:\n",
        "                WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CLASS_NAME, 'gemini-project-video')))\n",
        "                video_div = driver.find_element(By.CLASS_NAME, 'gemini-project-video')\n",
        "                iframe = video_div.find_element(By.TAG_NAME, 'iframe')\n",
        "                video_src = iframe.get_attribute('src') if iframe else None\n",
        "                if video_src and 'youtube.com/embed/' in video_src:\n",
        "                    video_id = video_src.split('/embed/')[1].split('?')[0]  \n",
        "                    video_link = f\"https://www.youtube.com/watch?v={video_id}\"\n",
        "                    break  \n",
        "            except Exception as e:\n",
        "                print(f\"Attempt failed for YouTube link on {url}: {e}\")\n",
        "                time.sleep(2)  \n",
        "\n",
        "        # Scraping \"What it does\"\n",
        "        try:\n",
        "            what_it_does_label = driver.find_element(By.XPATH, \"//p[text()='What it does']\")\n",
        "            what_it_does = what_it_does_label.find_element(By.XPATH, \"following-sibling::p\").text.strip()\n",
        "        except:\n",
        "            what_it_does = \"N/A\"\n",
        "\n",
        "        # Scraping \"Built with\"\n",
        "        try:\n",
        "            built_with_label = driver.find_element(By.XPATH, \"//p[text()='Built with']\")\n",
        "            built_with_list = built_with_label.find_element(By.XPATH, \"following-sibling::ul\")\n",
        "            built_with_items = [li.text.strip() for li in built_with_list.find_elements(By.TAG_NAME, 'li')]\n",
        "            built_with = ', '.join(built_with_items)\n",
        "        except:\n",
        "            built_with = \"N/A\"\n",
        "\n",
        "        # Scraping \"By\" (team member)\n",
        "        try:\n",
        "            by_label = driver.find_element(By.XPATH, \"//p[text()='By']\")\n",
        "            by = by_label.find_element(By.XPATH, \"following-sibling::p\").text.strip()\n",
        "        except:\n",
        "            by = \"N/A\"\n",
        "\n",
        "        # Scraping \"From\" (location)\n",
        "        try:\n",
        "            from_label = driver.find_element(By.XPATH, \"//p[text()='From']\")\n",
        "            location = from_label.find_element(By.XPATH, \"following-sibling::p\").text.strip()\n",
        "        except:\n",
        "            location = \"N/A\"\n",
        "\n",
        "        driver.quit()\n",
        "\n",
        "        # Return all scraped data as a dictionary\n",
        "        return {\n",
        "            'Title': title,\n",
        "            'Sub-Title': sub_title,\n",
        "            'YouTube Link': video_link,\n",
        "            'What it Does': what_it_does,\n",
        "            'Built With': built_with,\n",
        "            'By': by,\n",
        "            'Location': location,\n",
        "            'Project Link': url\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def scrape_projects_from_csv(input_csv, num_links=None, chunk_size=200):\n",
        "    project_data = []\n",
        "\n",
        "    # Read the project links from CSV and remove the first row\n",
        "    with open(input_csv, mode='r', newline='', encoding='utf-8') as file:\n",
        "        reader = csv.reader(file)\n",
        "        rows = list(reader)\n",
        "\n",
        "        data_rows = rows[1:]\n",
        "\n",
        "    # Define a function to process each row\n",
        "    def process_row(row):\n",
        "        project_url = row[0]\n",
        "        print(f\"Scraping data from: {project_url}\")\n",
        "        data = scrape_project_data(project_url)\n",
        "        if data:\n",
        "            return data\n",
        "        return None\n",
        "\n",
        "    # Use ThreadPoolExecutor to parallelize requests\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        results = executor.map(process_row, data_rows[:num_links] if num_links else data_rows)\n",
        "        project_data = [result for result in results if result]\n",
        "\n",
        "    # Split data into chunks and write to separate CSV files\n",
        "    for i in range(1, len(project_data), chunk_size):\n",
        "        chunk = project_data[i:i + chunk_size]\n",
        "        output_csv = f'/content/drive/MyDrive/project_data_{i // chunk_size + 1}.csv'\n",
        "        with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
        "            writer = csv.DictWriter(file, fieldnames=['Title', 'Sub-Title', 'YouTube Link', 'What it Does', 'Built With', 'By', 'Location', 'Project Link'])\n",
        "            writer.writeheader()\n",
        "            for data in chunk:\n",
        "                writer.writerow(data)\n",
        "\n",
        "scrape_projects_from_csv('/content/drive/MyDrive/project_links.csv', num_links=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mN0t8-hGrHJm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ourS0MRZGqUO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
